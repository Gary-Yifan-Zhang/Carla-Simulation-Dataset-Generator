# CARLA-KITTI Roadside Dataset Auto-Generator

![Python 3.6+](https://img.shields.io/badge/python-3.6%2B-blue)
![CARLA 0.9.12+](https://img.shields.io/badge/CARLA-0.9.12%2B-orange)

![LiDAR Point Cloud Visualization](samples/scene.png)  

<p align="center">
  <img src="framework.jpg" alt="System Architecture" width="600"/>
  <p align="center">Figure 1. System Architecture Diagram</p>
</p>

## üìö Table of Contents
1. [Project Background](#project-background)  
2. [Key Features](#key-features)
3. [Installation Guide](#installation-guide)
4. [Quick Start](#quick-start)
5. [Dataset Structure](#dataset-structure)
6. [Visualization Samples](#visualization-samples)
7. [Notes](#notes)
8. [Acknowledgements](#acknowledgements)

## üåü Project Background
This project is a high-quality roadside dataset auto-generator developed based on the CARLA simulation environment. It supports the following data outputs:
- **Multi-view RGB images** (resolution 1920√ó1080)
- **3D LiDAR point cloud data**
- **Annotation files** (including 2D/3D bounding boxes)
- **Sensor calibration parameters**

The generated data is fully compatible with the KITTI dataset format, making it suitable for training and validating autonomous driving perception algorithms.

## üåü Project Background
‚úÖ **Multi-sensor synchronized data collection**  
„ÄÄ‚îú‚îÄ Cameras (front/side/top multi-angles)  
„ÄÄ‚îú‚îÄ LiDAR (32-beam)  
„ÄÄ‚îî‚îÄ Inertial Measurement Unit  

‚úÖ **Intelligent traffic flow generation**  
„ÄÄ‚îú‚îÄ Random vehicle and pedestrian generation and control  
„ÄÄ‚îú‚îÄ Dynamic weather system  
„ÄÄ‚îî‚îÄ Support for complex road scenarios  

‚úÖ **Automatic annotation system**  
„ÄÄ‚îú‚îÄ Generates 2D/3D bounding box annotations  
„ÄÄ‚îú‚îÄ Supports object category labeling (vehicles/pedestrians/traffic signs, etc.)  
„ÄÄ‚îî‚îÄ Accurate position and orientation information  

## üõ†Ô∏è Installation Guide

### Environment Requirements
- **Python**: >= 3.6
- **CARLA Simulator**: >= 0.9.12

### Install Dependencies
1. Clone the repository:
   ```bash
   git clone https://github.com/Gary-Yifan-Zhang/Carla-Simulation-Dataset-Generator.git
   pip install -r requirements.txt

   # CARLA PythonAPIÂÆâË£ÖÂèÇËÄÉÂÆòÊñπÊñáÊ°£
   # https://carla.readthedocs.io/en/latest/build_system/
   ```
## üöÄ Quick Start


```python
# Step 1: Start CARLA server
./CarlaUE4.sh -quality-level=Epic

# tep 2: Run the generator script
python main.py 
```

## üìÇ Dataset Structure
```bash
dataset/
‚îú‚îÄ‚îÄ image_2/            # Multi-view RGB images
‚îú‚îÄ‚îÄ velodyne/           # LiDAR point cloud data
‚îú‚îÄ‚îÄ label_2/            # 2D/3D annotation files
‚îú‚îÄ‚îÄ calib/              # Sensor calibration parameters
‚îî‚îÄ‚îÄ tracklets/          # Object motion trajectory data
```

## üìù Data Format Specification
```python
"""
# Values Name Description
# Values    Name      Description
----------------------------------------------------------------------------
   1    type         Object type: 'Car', 'Pedestrian', 'Vehicles', 'Vegetation', 'TrafficSigns', etc.
   1    id           Unique ID for the object, -1 if not specified
   1    truncated    Float from 0 (non-truncated) to 1 (truncated), where
                     truncated refers to the object leaving image boundaries
   1    occluded     Integer (0,1,2,3) indicating occlusion state:
                     0 = fully visible, 1 = partly occluded
                     2 = largely occluded, 3 = unknown
   1    alpha        Observation angle of object, ranging [-pi..pi]
   4    bbox         2D bounding box of object in the image (0-based index):
                     contains left, top, right, bottom pixel coordinates
   3    dimensions   3D object dimensions: height, width, length (in meters)
   3    location     3D object location x,y,z in camera coordinates (in meters)
   1    rotation_y   Rotation ry around Y-axis in camera coordinates [-pi..pi]
   1    score        Only for results: Float, indicating confidence in
                     detection, needed for p/r curves, higher is better.
"""
```

### üì∑ Image Labels
- **Generation Method**: Generated by `is_visible_in_camera` function
- **Visibility Criteria**:
  - At least `MIN_VISIBLE_VERTICES_FOR_RENDER` vertices visible
  - At most `MAX_OUT_VERTICES_FOR_RENDER` vertices outside the image
- **Occlusion Determination**:
  - Fully visible: More than 6 vertices visible
  - Partially occluded: 4-5 vertices visible
  - Largely occluded: Less than 4 vertices visible
- **Truncation Calculation**: Number of vertices outside image / 8
- **Label Fields** (in KITTI standard format order):
  - `type`: Object type (e.g., 'Car', 'Pedestrian', etc.)
  - `id`: Unique object ID, -1 if not specified
  - `truncated`: Float from 0 (non-truncated) to 1 (truncated), indicating the extent the object leaves image boundaries
  - `occlusion`: Integer (0,1,2):
     - 0 = fully visible
     - 1 = partly occluded
     - 2 = largely occluded
  - `alpha`: Observation angle, fixed at 0 (no angle information in camera data)
  - `bbox`: 2D bounding box in image (0-based index): [left, top, right, bottom] pixel coordinates
  - `dimensions`: 3D object dimensions (height, width, length)
  - `location`: 3D object location (x, y, z) in camera coordinates
  - `rotation_y`: Rotation around Y-axis in camera coordinates


### üõ∞Ô∏è LiDAR Labels
- **Generation Method**: Generated by `is_visible_in_lidar` function
- **Visibility Criteria**: At least `MIN_VISIBLE_NUM_FOR_POINT_CLOUDS` point clouds belong to the target object
- **Fixed Values**:
  - `truncated`: 0 (LiDAR is not limited by image boundaries)
  - `occlusion`: 0 (LiDAR directly detects object surfaces)
  - `alpha`: 0 (LiDAR has no observation angle information)
  - `bbox`: [0, 0, 0, 0] (LiDAR has no 2D bounding box information)
  - **Label Fields** (in KITTI standard format order):
  - `type`: Object type (e.g., 'Car', 'Pedestrian', etc.)
  - `id`: Unique object ID, -1 if not specified
  - `truncated`: Float from 0 (non-truncated) to 1 (truncated), indicating the extent the object leaves image boundaries
  - `occlusion`: Integer (0,1,2):
     - 0 = fully visible
     - 1 = partly occluded
     - 2 = largely occluded
  - `alpha`: Observation angle, fixed at 0 (no angle information in camera data)
  - `dimensions`: 3D object dimensions (height, width, length)
  - `location`: 3D object location (x, y, z) in camera coordinates
  - `rotation_y`: Rotation around Y-axis in camera coordinates
  
## üé® Visualization Samples

### RGB Image with 3D Annotations
![RGB Image with 2D Bounding Boxes](samples/front_bbox_img.gif)  
*RGB Image with 2D Bounding Boxes*

### LiDAR Point Cloud Visualization
![LiDAR Point Cloud Visualization](samples/pointcloud_bbox.gif)  
*LiDAR Point Cloud Visualization*

## üìù Notes
1. **The CARLA server must remain running** during data generation.
2. **A high-performance GPU is recommended** (e.g., 1080Ti or higher) for optimal performance.
3. **Data generation rate** is approximately **8-12 FPS**, depending on hardware configuration.
4. **Custom sensor layouts are supported**. Modify `config/sensors.json` to configure sensor placement.

## üôè Acknowledgements
This project is developed based on the following open-source projects:  
- [CARLA Simulator](https://carla.org/)  
- [mmmmaomao/DataGenerator](https://github.com/mmmmaomao/DataGenerator)  
- [KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/)  

## üìÖ TODO List
- [ ] Further modify data types
- [ ] Add bounding boxes to multi-view images
- [ ] Multi-radar fusion
- [ ] Create more complex scenarios using OpenScenario
- [ ] Migrate to CARLA with UE5


---

üîÑ **Continuously updated** | üìß **Issue reporting**: [Yifan Zhang](Gary.Yifan.Zhang@gmail.com)  
‚≠êÔ∏è **If this project is helpful to you, please give it a star on GitHub!**
